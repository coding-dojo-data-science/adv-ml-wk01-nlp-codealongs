{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99451153-2415-4ec0-836f-a50c92f89e48",
   "metadata": {},
   "source": [
    "# Analysis and Classification with Natural Language [Amazon Reviews Version]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a91fe-74d7-4e97-a7ee-55a0ef0c7a14",
   "metadata": {},
   "source": [
    "In this notebook we will continue with the data we prepared in the last lecture.  Our goals will be to separate the data into positive and negative tweets, compare and analyze them to notice any differences, and then to create a model to classify future tweets as having a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41670d9-9e1a-4e5e-89d2-c8c478d33d87",
   "metadata": {},
   "source": [
    "> See the version of Lecture 1 using this new dataset in the \"Lecture 1-Amazon/\" folder: `AmazonVers-W01L01-working-with-text-data-v2.ipynb`. Included: using Regex to remove raw HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec56ce-d948-47e0-99a5-71fd5d07ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open(\"../Data-AmazonReviews/Amazon Product Reviews.md\") as f:\n",
    "    info = f.read()\n",
    "\n",
    "display(Markdown(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed38e7ae-4cb5-41b0-a32e-21d788940a15",
   "metadata": {},
   "source": [
    "# Loading Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d319495-f696-425a-ad7a-735f0bc939cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "pd.set_option('display.max_colwidth', 300) # Amazon reviews are MUCH longer than tweets, so can't do None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c606ed8-8b5e-4fbe-b999-84da1c26f51e",
   "metadata": {},
   "source": [
    "# Import Processed Data\n",
    "\n",
    "We created several versions of our text in the previous lecture.  We will load those again to use for analysis and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633e30c-2b18-4115-8b4b-7f437f16838f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load processed data\n",
    "# df = pd.read_csv('../Data/processed_data.csv')\n",
    "df = pd.read_csv(\"../Data-AmazonReviews/processed_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b90b7-1bf7-4665-b280-17a9bcf03d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80500263-47b3-46d5-aeb7-15fcba109602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_lemmas = df.loc[0,'spacy_lemmas']\n",
    "test_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f2c62-5996-4f50-88de-7a76cd88cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check type of first row\n",
    "type(test_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac66cd2-b247-41ed-97a8-f953eb7b6fbd",
   "metadata": {},
   "source": [
    "## Convert strings of lists to lists\n",
    "\n",
    "You might recall that pandas interprets lists and strings when importing data.  We can define a quick function to convert them back.\n",
    "\n",
    "How can we use string methods to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c20729-171b-4b7f-a4e2-de0b40c17e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to convert string lists back to lists\n",
    "\n",
    "def convert_to_list(string):\n",
    "    \"\"\"Remove braces, split tokens on commas, and then strip quotes from outside of each token\"\"\"\n",
    "    new_list = [token.strip(\"' \") for token in string.strip(\"[]\").split(',')]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0c0be-e5a1-4daf-b545-0f655b3c5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## applymap function to convert string lists back to lists.\n",
    "\n",
    "cols = ['tokens','no_stops','no_stops_no_punct', 'spacy_lemmas', 'bigrams']\n",
    "\n",
    "df[cols] = df[cols].applymap(convert_to_list)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034e4bb-fcd4-409e-8083-70de772d2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check type of first row\n",
    "type(df.loc[0,'spacy_lemmas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079b571-1f18-4a6a-9329-5b1612965a85",
   "metadata": {},
   "source": [
    ">Alternatively, the saved joblib file version does not need any additional preprocessing, the lists are still lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39514ac-af34-4115-900c-bf0b113cb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "df = joblib.load('../Data-AmazonReviews/processed_data.joblib')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f1f52-f645-4591-81ef-b93609bad1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check type of first row\n",
    "test_lemmas = df.loc[0,'spacy_lemmas']\n",
    "test_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ad4cc-caf9-4b77-915a-9843425fa392",
   "metadata": {},
   "source": [
    "## Creating Groups for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ace48-be20-4ea2-8db2-abd416dbe9f7",
   "metadata": {},
   "source": [
    "Amazon Reviews are out of 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca23730-426d-4f32-b2f0-f104c07ce5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking what values are in the overall ratings\n",
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f503a9-b7e4-4976-a0c7-f50e4bb4ffcd",
   "metadata": {},
   "source": [
    "To understand what customers do and do not like about Hoover products, we will define 2 groups:\n",
    "- High Ratings\n",
    "    - Overall rating = 5.0\n",
    "- Low Ratings\n",
    "    - Overall rating = 1.0 or 2.0\n",
    "\n",
    "\n",
    "We can use a function and .map to define group names based on the numeric overall ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b993-547d-4ae4-b678-12d73a13f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(x):\n",
    "    if x>=5.0:\n",
    "        return \"high\"\n",
    "    elif x <=2.0:\n",
    "        return \"low\"\n",
    "    else: \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7137161-44cf-424c-9f25-07a2a9082578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return high\n",
    "create_groups(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9df18-1f9c-48e4-9f1f-e4e0aa88ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return low\n",
    "create_groups(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8928c6-120a-445a-a398-b5a610fb3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return nothing\n",
    "create_groups(4.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed1fe3-42c6-4d6e-99fb-8d94df027bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the function to create a new \"rating\" column with groups\n",
    "df['rating'] = df['overall'].map(create_groups)\n",
    "df['rating'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd4eed-7019-4267-aa1a-bf72dc384b0c",
   "metadata": {},
   "source": [
    "## Class Balance\n",
    "\n",
    "It's always a good idea to check the class balance when creating a classification model.  This can affect modeling bias and interpretation of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445605e2-8189-4dce-8388-a08e9945ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check class balance of 'rating'\n",
    "df['rating'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc89e5c-661e-4940-aa90-be290fa2d0ef",
   "metadata": {},
   "source": [
    "# Divide by Group\n",
    "\n",
    "Since we want to compare and classify high rating reviews vs low-rating reviews, we will create 2 different dataframes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa08b5-904e-4e55-8b80-99e1409e930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide documents by rating\n",
    "high = df.loc[df['rating'] == 'high']\n",
    "low = df.loc[df['rating'] == 'low']\n",
    "print('high ratings')\n",
    "display(high.head())\n",
    "print('low ratings')\n",
    "display(low.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094d71cf-6d27-45e6-88b4-2cd36cc79fd8",
   "metadata": {},
   "source": [
    "# Length\n",
    "\n",
    "One easy way to compare text is to compare the length of each text.  We could compare the number of character or tokens.  In this case we will compare the number of characters in each tweet for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009c793-416f-4de4-b078-ae53377bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize median review length\n",
    "high_len = high['length'].median()\n",
    "low_len = low['length'].median()\n",
    "\n",
    "ax = sns.barplot(data=df, x='rating', y='length', estimator='median',);\n",
    "\n",
    "# Show plot before print statement\n",
    "plt.show()\n",
    "print(f' The median character length for {low_len} for low Ratings and {high_len} for high ratings.')\n",
    "\n",
    "# Save figure\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3682fe-8a3b-4977-a047-29c162907336",
   "metadata": {},
   "source": [
    "Low rating reviews are generally longer than high-rating reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f8c05-a02f-4c8a-aa94-42c33c090f55",
   "metadata": {},
   "source": [
    "# Frequency Distribution\n",
    "\n",
    "Let's take a look at how often different words appear in the tweets.  \n",
    "\n",
    "The NLTK FreqDist class expects a list of all tokens in all documents.  We can create this with the `.explode()` method to create new rows for every word.  We can then convert the resulting series to a list with `to_list()` to pass to the `FreqDist` class constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d35609-ca3b-4b3c-8e37-0da580c50531",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of all tokens in all documents\n",
    "high_words = high['tokens'].explode().to_list()\n",
    "low_words = low['tokens'].explode().to_list()\n",
    "high_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c656a-77b7-4534-b337-45d949830c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate high frequency distribution\n",
    "high_freq_dist = FreqDist(high_words)\n",
    "low_freq_dist = FreqDist(low_words)\n",
    "\n",
    "## Plot the distribution\n",
    "high_freq_dist.plot(20, title='High Rating Token Distribution')\n",
    "\n",
    "low_freq_dist.plot(20, title='Low Rating Token Frequency Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b41c75-5de6-4da0-a585-5b9ffd5d59c1",
   "metadata": {},
   "source": [
    "We can see already that punctuation and very common words are at the top of both lists.  To avoid this, we can use our normalized data instead.  Let's use our lemmas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f653d8-3435-419e-bf47-ddaab4cc9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of all tokens in all documents\n",
    "high_words = high['spacy_lemmas'].explode().to_list()\n",
    "low_words = low['spacy_lemmas'].explode().to_list()\n",
    "\n",
    "## Instantiate high frequency distribution\n",
    "high_freq_dist = FreqDist(high_words)\n",
    "low_freq_dist = FreqDist(low_words)\n",
    "\n",
    "## Plot the distribution\n",
    "high_freq_dist.plot(20, title='high Rating Token Distribution')\n",
    "\n",
    "low_freq_dist.plot(20, title='low Rating Token Frequency Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89055862-2ed0-44ea-be47-e7324d65cc93",
   "metadata": {},
   "source": [
    "# Word Clouds\n",
    "\n",
    "Word Clouds tell us the same kinds of things as a frequency distribution, but are a nice way to visualize.  They can be used for project headers or presentations.\n",
    "\n",
    "The `WordCloud` class expects texts to be one long string, so.  We will use our lemmas again, so we have to join all the tokens for each document into one long string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14adc25f-132a-48fe-8446-7c43cec19938",
   "metadata": {},
   "source": [
    "## Processing the data for WordCloud\n",
    "\n",
    "WordCloud expects one single string of all the words in a corpus.  since we already have a list of all the words, for each sentiment, we can just join those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769dbeb-8ded-42b5-8add-bcb647c6244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join the original text for each group\n",
    "high_text_joined = \" \".join(high['text'])\n",
    "low_text_joined = \" \".join(low['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c0305-a972-4d66-9b9e-40188bc09bc0",
   "metadata": {},
   "source": [
    "## Creating and displaying the Word Clouds\n",
    "\n",
    "We will create the clouds and generate the images below.  We want to focus on larger words, otherwise we get some strange results due to contractions.  We will tell WordCloud to only generate words with 2 or more letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1063a-8f15-4e18-8653-cb6191dac812",
   "metadata": {},
   "source": [
    "### Using the Original Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f50b33-184b-41ec-8a6e-661b4c7b6270",
   "metadata": {},
   "source": [
    "Let's start with using the raw text to make the wordclouds. We've provided a helper function for plotting the two wordclouds side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fab243f-4bb4-4fd0-9802-0499b1ac0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_wordclouds(high_cloud, low_cloud, title='Comparing Word Usage'):\n",
    "    \"\"\"Plots the wordlcouds for our two groups\"\"\"\n",
    "    ## Plot the Images\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    axes[0].imshow(high_cloud)\n",
    "    axes[0].set_title('High Ratings')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(low_cloud)\n",
    "    axes[1].set_title('Low Ratings')\n",
    "    axes[1].axis('off')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    fig.suptitle(title,y=1.0, fontsize=20);\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c9623-e01c-4375-8378-d33fe420dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the WordCloud Images\n",
    "wordcloud_kws = dict(min_word_length=2,width=800, height=600, random_state=42)\n",
    "high_cloud = WordCloud(**wordcloud_kws, colormap='Greens').generate(high_text_joined)\n",
    "low_cloud = WordCloud(**wordcloud_kws, colormap='Reds').generate(low_text_joined)\n",
    "\n",
    "fig = plot_wordclouds(high_cloud, low_cloud, title=\"Comparing Original Reviews\")\n",
    "\n",
    "# Saving figure for README\n",
    "fig.savefig('images/wordclouds-original-reviews.png', dpi=300, bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab048d7-0406-4ee2-b79f-366944bc7592",
   "metadata": {},
   "source": [
    "> Consider what words should be considered stopwords **just for EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7aeab-b224-4e5b-a570-40c6ae0058dc",
   "metadata": {},
   "source": [
    "### Removing Stopwords for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7482dfa-2429-45cd-8e07-8d5269607f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more custom stopwords for EDA\n",
    "from wordcloud import STOPWORDS\n",
    "#custom_stopwords = [*STOPWORDS, ]\n",
    "custom_stopwords = [*STOPWORDS,'use','Hoover','machine','clean','vacuum','carpet','cleaner','one','two','five','star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543dea38-03f3-420e-9d45-a05a5894d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the WordCloud Images using the custom stopwords\n",
    "wordcloud_kws = dict(min_word_length=2,width=800, height=600, random_state=42,\n",
    "                     stopwords=custom_stopwords)\n",
    "high_cloud = WordCloud(**wordcloud_kws, colormap='Greens').generate(high_text_joined)\n",
    "low_cloud = WordCloud(**wordcloud_kws, colormap='Reds').generate(low_text_joined)\n",
    "\n",
    "# ## Plot the Images\n",
    "fig =  plot_wordclouds(high_cloud,low_cloud, title=\"Comparing Original Reviews - Custom Stopwords\")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig('images/wordclouds-original-reviews-custom-stop.png', dpi=300,transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5929ec0-a3ce-4f75-a339-a339cc686744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb227ed8-b773-4390-987f-eaef969d7dd6",
   "metadata": {},
   "source": [
    "### Using Lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eaa1e6-eaf6-44d9-879c-03f548049323",
   "metadata": {},
   "source": [
    "Depending on the corpus, visualizing lemmas may provide a clearer view of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf507096-a6b2-4588-9371-e134f2b351d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join the words for each sentiment\n",
    "all_high_lemmas = ' '.join(high_words)\n",
    "all_low_lemmas = ' '.join(low_words)\n",
    "type(all_high_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af340f4e-20ec-470e-a087-f93d02c97c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate the WordCloud Images\n",
    "# wordcloud_kws = dict(min_word_length=2,width=800, height=600, )\n",
    "high_cloud = WordCloud(**wordcloud_kws, colormap='Greens').generate(all_high_lemmas)\n",
    "low_cloud = WordCloud(**wordcloud_kws, colormap='Reds').generate(all_low_lemmas)\n",
    "\n",
    "## Plot the Images\n",
    "fig = plot_wordclouds(high_cloud, low_cloud, title='Comparing Lemmas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be99d0-98ab-4ac0-9df9-29992a0df2f6",
   "metadata": {},
   "source": [
    "This can be a fun way to help you stakeholders get a feel for the token distributions in our data. However, its hard to truly quantify the things that customers did or did not like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e677b95-9745-4716-aaa3-3b0b90f34ca0",
   "metadata": {},
   "source": [
    "# N-gram Analysis\n",
    "\n",
    "We can do something similar with ngrams.  NLTK has native classes for finding and measuring the frequency of ngrams as well.\n",
    "\n",
    "For this we need 2 objects: a measures class depending on the measure we want to see and a finder class based on the number of words in our ngram.\n",
    "\n",
    "These classes exist for bi, tri, and quadgrams, we will start with bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecc105e-7855-4f71-aad9-5067b916cac0",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327fef5-f4b7-4438-b98c-d6c361b3f378",
   "metadata": {},
   "source": [
    "### Calcuate Frequency Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e5fb6-19a5-482e-825e-5b64e47fb8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "## Create measure classes\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "## Create collector classes\n",
    "high_finder = BigramCollocationFinder.from_words(high_words)\n",
    "low_finder = BigramCollocationFinder.from_words(low_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f8899-8ba1-4f35-a98e-8ffe4aba8fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate high-rating ngram scores\n",
    "high_ngram_scores = high_finder.score_ngrams(measures.raw_freq)\n",
    "\n",
    "# Save as a dataframe\n",
    "high_ngram_scores_df = pd.DataFrame(high_ngram_scores, columns=['high ngram', 'high score'])\n",
    "\n",
    "\n",
    "## Calculate low-rating ngram scores\n",
    "low_ngram_scores = low_finder.score_ngrams(measures.raw_freq)\n",
    "low_ngram_scores_df = pd.DataFrame(low_ngram_scores, columns=['low ngram', 'low score'])\n",
    "\n",
    "## Display the frequency scores\n",
    "display(high_ngram_scores_df.head(20),low_ngram_scores_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20aeb4-08c3-4641-8ddc-921e2f15cc8b",
   "metadata": {},
   "source": [
    "#### Plot Frequency Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae41261-0cb6-4337-b491-9d065e274505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_group_ngrams( low_ngram_scores, high_ngram_scores, \n",
    "                       plot_col_low=\"low score\",top_n=20,\n",
    "                      plot_col_high=\"high score\",figsize=(12, 8),):\n",
    "    # Get top n ngrams for both groups (set index for easier pandas plotting)\n",
    "    top_n_ngrams_high = high_ngram_scores.set_index(\"high ngram\").head(top_n)\n",
    "    top_n_ngrams_low = low_ngram_scores.set_index(\"low ngram\").head(top_n)\n",
    "\n",
    "    ## Plot the ngram frequencies\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    #\n",
    "    top_n_ngrams_high[plot_col_high].sort_values().plot(\n",
    "        kind=\"barh\", title=\"High-Rating Ngram Frequency\", ax=axes[0], color=\"green\"\n",
    "    )\n",
    "\n",
    "    top_n_ngrams_low[plot_col_low].sort_values().plot(\n",
    "        kind=\"barh\", title=\"Low-Rating Ngram Frequency\", color=\"crimson\", ax=axes[1]\n",
    "    )\n",
    "    for ax in axes:\n",
    "        ax.spines[\"top\"].set_visible(False)  # Remove the top spine\n",
    "        ax.spines[\"right\"].set_visible(False)  # Remove the right spine\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f6805-f68b-413e-86ba-c77330010367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ngram frequecies using the function\n",
    "with sns.plotting_context('talk'):\n",
    "    fig = plot_group_ngrams(low_ngram_scores_df, high_ngram_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591d4f8-0b67-4e3c-adcb-368b260601d8",
   "metadata": {},
   "source": [
    "This might be more enlightening than the individual word frequency counts.  There are some relatable bigrams here.\n",
    "\n",
    "We are also seeing a lot of versions of 'happy mother's day' which may tell us that more normalization is required here.  This would be a consideration in longer term projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089bc666-19dd-4fb5-8e19-705f1711f06b",
   "metadata": {},
   "source": [
    "## Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad40cf-b69a-43ef-95d2-d2b964efd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
    "\n",
    "## Create measure classes\n",
    "measures = TrigramAssocMeasures()\n",
    "\n",
    "## Create collector classes\n",
    "high_finder = TrigramCollocationFinder.from_words(high_words)\n",
    "low_finder = TrigramCollocationFinder.from_words(low_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a7876-a27c-4706-8495-e4f18f05fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate high-rating trigram scores\n",
    "high_ngram_scores = high_finder.score_ngrams(measures.raw_freq)\n",
    "# Save as a dataframe\n",
    "high_ngram_scores_df = pd.DataFrame(high_ngram_scores, columns=['high ngram', 'high score'])\n",
    "\n",
    "\n",
    "\n",
    "## Calculate low-rating ngram scores\n",
    "low_ngram_scores = low_finder.score_ngrams(measures.raw_freq)\n",
    "# Save as a dataframe\n",
    "low_ngram_scores_df = pd.DataFrame(low_ngram_scores, columns=['low ngram', 'low score'])\n",
    "\n",
    "\n",
    "## Display the frequency scores\n",
    "display(high_ngram_scores_df.head(20),low_ngram_scores_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab22d6d-99e4-445b-b0be-2de80ddb3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trigram frequecies using the function\n",
    "# with plt.style.context(['ggplot','dark_background',]):\n",
    "with sns.plotting_context('talk'):\n",
    "\n",
    "    fig = plot_group_ngrams(low_ngram_scores_df, high_ngram_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04d25d-336e-425b-947b-a3fe1eb79ea9",
   "metadata": {},
   "source": [
    "# Preparing Data for Modeling\n",
    "\n",
    "In this lesson we will be using a Bag of Words approach to modeling.  This means our final features will be just the counts of how many times each word appears in each document.  \n",
    "\n",
    "We have many versions of our data to choose from for modeling.  Our lemmas are probably the most normalized, but our raw tokens have the most information.  The others are somewhere in between.  In this notebook we will be using the lemmatized data.  \n",
    "\n",
    "We will try both the CountVectorizer and TfidfVectorizer for vectorization.  Both create a column for every word in the vocabulary.\n",
    "\n",
    "**CountVectorizer** adds the raw counts of each word for each document.  It can be very sensitive to stop words, so we want to be sure to \n",
    "\n",
    "**TfidifVectorizer** adds a value that represents how unique the word is to the document, compared to all other documents in the corpus.  The more times a word appears in a document and the fewer times it appears in other documents the higher the value will be for that word.\n",
    "\n",
    "Both of these vectorizers assume that documents will be single strings.  We will join our lemmas together again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872785c7-86de-44f8-99b5-d38c0d62cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Modeling Package\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdab312-7ab2-450a-80f2-e5ecd88415bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classification_metrics(y_true, y_pred, label='',\n",
    "                           output_dict=False, figsize=(8,4),\n",
    "                           normalize='true', cmap='Blues',\n",
    "                           colorbar=False,values_format=\".2f\"):\n",
    "    \"\"\"Modified version of classification metrics function from Intro to Machine Learning.\n",
    "    Updates:\n",
    "    - Reversed raw counts confusion matrix cmap  (so darker==more).\n",
    "    - Added arg for normalized confusion matrix values_format\n",
    "    \"\"\"\n",
    "    # Get the classification report\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    \n",
    "    ## Print header and report\n",
    "    header = \"-\"*70\n",
    "    print(header, f\" Classification Metrics: {label}\", header, sep='\\n')\n",
    "    print(report)\n",
    "    \n",
    "    ## CONFUSION MATRICES SUBPLOTS\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n",
    "    \n",
    "    # Create a confusion matrix  of raw counts (left subplot)\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=None, \n",
    "                                            cmap='gist_gray_r',# Updated cmap\n",
    "                                            values_format=\"d\", \n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[0]);\n",
    "    axes[0].set_title(\"Raw Counts\")\n",
    "    \n",
    "    # Create a confusion matrix with the data with normalize argument \n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred,\n",
    "                                            normalize=normalize,\n",
    "                                            cmap=cmap, \n",
    "                                            values_format=values_format, #New arg\n",
    "                                            colorbar=colorbar,\n",
    "                                            ax = axes[1]);\n",
    "    axes[1].set_title(\"Normalized Confusion Matrix\")\n",
    "    \n",
    "    # Adjust layout and show figure\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return dictionary of classification_report\n",
    "    if output_dict==True:\n",
    "        report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "        return report_dict\n",
    "\n",
    "def evaluate_classification(model, X_train, y_train, X_test, y_test,\n",
    "                         figsize=(6,4), normalize='true', output_dict = False,\n",
    "                            cmap_train='Blues', cmap_test=\"Reds\",colorbar=False):\n",
    "  # Get predictions for training data\n",
    "  y_train_pred = model.predict(X_train)\n",
    "  # Call the helper function to obtain regression metrics for training data\n",
    "  results_train = classification_metrics(y_train, y_train_pred, #verbose = verbose,\n",
    "                                     output_dict=True, figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_train,\n",
    "                                     label='Training Data')\n",
    "  print()\n",
    "  # Get predictions for test data\n",
    "  y_test_pred = model.predict(X_test)\n",
    "  # Call the helper function to obtain regression metrics for test data\n",
    "  results_test = classification_metrics(y_test, y_test_pred, #verbose = verbose,\n",
    "                                  output_dict=True,figsize=figsize,\n",
    "                                         colorbar=colorbar, cmap=cmap_test,\n",
    "                                    label='Test Data' )\n",
    "  if output_dict == True:\n",
    "    # Store results in a dataframe if ouput_frame is True\n",
    "    results_dict = {'train':results_train,\n",
    "                    'test': results_test}\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616cb97-93b1-4dba-b49a-a2da3aafddca",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516ce1a-ab99-49d9-9f07-cf6109395c4b",
   "metadata": {},
   "source": [
    "We created null values in our target column, so we will drop the null values from the rating column before making our X and y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb22591-bb34-4e7f-aff6-f270f5033b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df_ml without null ratings\n",
    "df_ml = df.dropna(subset=['rating'])\n",
    "df_ml.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f2f9e-ce50-4afb-94f1-5282c91bc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "## X - Option A)  lemmas\n",
    "# def join_tokens(token_list):\n",
    "#     joined_tokens = ' '.join(token_list)\n",
    "#     return joined_tokens\n",
    "# X = df_ml['spacy_lemmas'].apply(join_tokens)\n",
    "\n",
    "# X - Option B) original raw text\n",
    "X = df_ml['text']\n",
    "\n",
    "# y - use our binary target \n",
    "y = df_ml['rating']\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de6eba-2f88-4fd2-ad3d-cc5fa3d1404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309780d-b1cd-4ae0-bccd-6305cd04deca",
   "metadata": {},
   "source": [
    "### Validation Split\n",
    "\n",
    "We will perform a triple split.  That way we can tune our model to a validation set and test the final version on a test set.  This avoid tuning our model to specifically perform well on just one set of testing data and gives us a better idea of how our model will perform on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f42d22-51aa-426b-971b-db7cccbd45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train-test split (no val) \n",
    "# X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "# X_train_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f0fca-87a6-4b79-a087-e31f6bc86f67",
   "metadata": {},
   "source": [
    "- Using 70% of data as train, 15% as test and 15 % as val (will use in deep nlp codealongs next class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088deb45-a0a7-4089-8468-ee7579e260e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data into train, test, val\n",
    "\n",
    "# Create a 70/30 train-split \n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=.5, random_state=42)\n",
    "\n",
    "#\n",
    "(len(X_train_full), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6de584-9241-442b-8bd6-410e77d8df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance\n",
    "y_train_full.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d79efd-0465-4e6e-9bec-1a4fcd901fac",
   "metadata": {},
   "source": [
    "### 🕹️ Under-Sampling Majority Group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4541c4a-048a-4298-9e28-248be0618ff5",
   "metadata": {},
   "source": [
    "We have a very imbalanced dataset. We will take a small sample from the majority class to match the number of reviews for the minority group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946a285-abe6-4d8e-b068-f98572a9d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07d408-43f4-41b0-9c55-2f9cd12de8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a RandomUnderSampler\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# Fit_resample on the reshaped X_train data and y-train data\n",
    "X_train, y_train = sampler.fit_resample(X_train_full.values.reshape(-1,1),y_train_full)\n",
    "\n",
    "# Flatten the reshaped X_train data back to 1D\n",
    "X_train = X_train.flatten()\n",
    "\n",
    "# Check for class balance\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289a2fd-3ccc-49f8-bd8f-d4d198002828",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We are going to use a Multinomial Naive Bayes model.  Bayesian models have been shown to often perform well with text data.  However, we could use any classification model we wanted to.\n",
    "\n",
    "We will be using a **Bag of Words** approach to classififying this text.  It's called **Bag of Words** because it's like we just put the words for each document into a bag.  We will not respect the order of the words, only which words are in the text and how many times they appear.  The models will use the frequency of each word in each text to classify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9931e-6be0-49fb-99fb-0ff398e7e495",
   "metadata": {},
   "source": [
    "## Data Preparation: Vectorization\n",
    "\n",
    "We have texts that are variable in length, but our sklearn models require a standard input size.  How do we make this transformation?\n",
    "\n",
    "Then answer is Vectorization!\n",
    "\n",
    "We will be using 2 different forms of vectorization: Count Vectorization and TF-IDF Vectoriation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf027b-a67a-4e24-a6f3-96d2a0fcf3aa",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "\n",
    "This form of vectorization is the easier one to understand.  We will use the vocabulary of all unique token found in our training data and create a column for each one.  As you can image, this can be a lot of columns!  This is one reason that we want to normalize data and remove stop words.\n",
    "\n",
    "We've already lemmatized our data and removed stop words, but if we hadn't CountVectorizer could do that for us.  It can also create ngrams, strip accent marks, and more.  \n",
    "\n",
    "One new feature we will use is `min_df`.  This will remove words that only appear a few times.  Remember that models need many examples of feature values in order to find patterns.  Tokens that only appear once or twice in the data will not be useful for our models to learn from.  Removing them will further normalize the data and reduce the number of features.\n",
    "\n",
    "We will also try including bigrams in our columns.  We can tell the vectorizer the range of ngrams to use with `ngram_range=`.  It takes a tuple of a lower and upper bound of ngrams.  For example, if we wanted unigrams (one token), bigrams, and trigrams, we would set `ngrame_range=(1,3)`.  In this case we will just use unigrams and bigrams.\n",
    "\n",
    "[Here is the documention for CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to learn more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04158d-018e-49d1-adc6-84f06a41fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate CountVectorizer\n",
    "countvector = CountVectorizer()#min_df=3, ngram_range=(1,2))\n",
    "countvector.fit(X_train)\n",
    "\n",
    "# Transform X_train to see the result (for demo only)\n",
    "countvector.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad77d6-6daf-4d32-823b-8f3a5f3c0e1e",
   "metadata": {},
   "source": [
    "It would be great to examine the resulting data, but as you can see it's saved as a **sparse matrix**.  We can also see that there are over 100 thousand columns, so they would be difficult to explore.\n",
    "\n",
    "A **Sparse Matrix** is a compressed form of a numpy array.  It sqeezes out all of the 0 values to save space.  Otherwise our vectorized dataset would be very large!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c38fb2-2a6b-4af7-ad3e-ee376d66e889",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Like other transformers, sklearn Vectorizers can be used in pipelines with models.  We will create a pipeline with our vectorizer and our Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6d4e9-30fc-472a-8cad-3485d8927e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a model pipeline \n",
    "nbayes = MultinomialNB()\n",
    "\n",
    "count_pipe = Pipeline([('vectorizer', countvector), \n",
    "                       ('bayes', nbayes)])\n",
    "\n",
    "count_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899e962-021f-4e9a-a367-495cffd1017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate count_pipe\n",
    "evaluate_classification(count_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d5b086-396c-45f9-b558-36746f1e32b9",
   "metadata": {},
   "source": [
    "Our model was 66% accurate, but did not seem overly biased against any given class.  There might be a slight bias toward the neutral class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c085c2-a186-484c-bd6d-23dedeef782d",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "Another popular **Bag of Words** vectorization strategy is TF-IDF vectorization.  In this stragegy, rather than just counting each word, we compare the frequency of a word in a document with the frequency of that word in other documents.  This measures the specificity of the word.\n",
    "\n",
    "If a word is common in a document, but rare in the corpus as a whole, it gets a higher value.  If it's common throughout the corpus, it gets a lower value.  This helps TF-IDF vectorized data to resist the effects of common stop words or other words that tend to be common in that particular corpus.  They will have a lesser effect on the prediction.\n",
    "\n",
    "[TfidfVectorizer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7561ab-9586-46c6-a61a-7b6e2a479bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate TF-IDF Vectorizor\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "## Instantiate model\n",
    "tfidfbayes = MultinomialNB()\n",
    "\n",
    "\n",
    "## Create pipeline: tfidf_pipe\n",
    "tfidf_pipe = Pipeline([('vectorizer', tfidf),\n",
    "                       ('bayes', tfidfbayes)])\n",
    "\n",
    "\n",
    "\n",
    "## Fit pipeline\n",
    "tfidf_pipe.fit(X_train, y_train)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536d2b9-4e3b-4917-860e-4240cc0050c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Evaluate the tfidf_pipeline model\n",
    "evaluate_classification(tfidf_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f2588-1d5d-4be1-b766-30ce1ce42526",
   "metadata": {},
   "source": [
    "In this case we are seeing an overall reduction in model accuracy.  The CountVectorizer seems to have been the better choice.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "There are a lot of choices in how to prepare data for text classification:\n",
    "* Remove stop words and punctuation?\n",
    "* Lemmatize?  Stem?\n",
    "* How to Vectorize?\n",
    "* ngrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69a907-a9e6-4033-bca9-e346eceece78",
   "metadata": {},
   "source": [
    "## Challenge:\n",
    "\n",
    "Try improving the score.  Ideas to try:\n",
    "* Try a different version of the data: non-lemmatized data, include stop words, different range of n_grams\n",
    "* Tune the vectorizer: Increase the min_df, decrease the max_df, other options: [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "* Tune the model: [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "* Try a different model: Can use any classification model for this.\n",
    "* Consider PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d692a-7b77-4bd1-a688-6469b5ba2863",
   "metadata": {},
   "source": [
    "# (Bonus/Optional) GridSearch Text Preprocessing Params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198bf08-8f2e-4780-bc7e-bff4c628a444",
   "metadata": {},
   "source": [
    "We can tune our text preprocessing choices using gridsearch. The same preprocessing options may not work well for different types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e17ce-9c70-458e-80ff-453ef084341b",
   "metadata": {},
   "source": [
    "#### GS MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a93052-6ad3-47df-9bd2-340eb1f9fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_pipe = Pipeline([('vectorizer',CountVectorizer()),\n",
    "                    ('clf',MultinomialNB())])\n",
    "gs_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e33da-93ef-4387-9ae4-f1044075119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params to try for both vectorizers\n",
    "param_grid_shared = {\n",
    "    \"vectorizer__max_df\": [0.7, 0.8, 0.9],\n",
    "    'vectorizer__min_df': [ 2, 3, 4 ], \n",
    "    \"vectorizer__max_features\": [None, 1000, 2000],\n",
    "    \"vectorizer__stop_words\": [None,'english']\n",
    "}\n",
    "\n",
    "# Setting params for the count vectorizer\n",
    "param_grid_count = {\n",
    "    'vectorizer':[CountVectorizer()],\n",
    "    **param_grid_shared\n",
    "}\n",
    "\n",
    "\n",
    "# Setting params for tfidf vectorizer \n",
    "param_grid_tfidf = {\n",
    "    'vectorizer': [TfidfVectorizer()],\n",
    "    \"vectorizer__norm\": [\"l1\", \"l2\"],\n",
    "    \"vectorizer__use_idf\": [True, False],\n",
    "    **param_grid_shared\n",
    "}\n",
    "\n",
    "# combine into list of params\n",
    "params_combined = [param_grid_count, param_grid_tfidf]\n",
    "params_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74435b8c-ced0-44de-9e0f-e4e13036633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(gs_pipe, params_combined, cv=3, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fe2ff-8182-47b1-ab5d-91fba51e20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best estimator\n",
    "best_gs_pipe = grid_search.best_estimator_\n",
    "evaluate_classification(best_gs_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd07613-a6af-4418-bfc5-8f9392ac4b5d",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c90d2-ea9a-42ed-aab6-5e1d380c9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_pipe  = Pipeline([('vectorizer',CountVectorizer()),\n",
    "                    ('clf',RandomForestClassifier(class_weight='balanced'))])\n",
    "rf_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e1113-7c76-4a0b-bcf0-fba1f49c13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(rf_pipe, params_combined, cv=3, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40773e62-cbf1-4da8-ad2b-09248a885afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_pipe = grid_search.best_estimator_\n",
    "evaluate_classification(best_rf_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b4d64-f8bb-42f2-8337-1cc037d813a8",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935f3dce-cbd3-4a65-a1db-85677e506c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_pipe  = Pipeline([('vectorizer',CountVectorizer()),\n",
    "                    ('clf',LogisticRegression(max_iter=500, class_weight='balanced'))])\n",
    "# logreg_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63272c6f-8d33-48a1-bd63-58d74d647f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create grid search\n",
    "grid_search = GridSearchCV(logreg_pipe, params_combined, cv=3, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    \n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0087b5-a67c-4b97-85e6-7965c2ad371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg_pipe = grid_search.best_estimator_\n",
    "evaluate_classification(best_logreg_pipe, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768bff2-3556-4f7e-b3fa-79f36776a256",
   "metadata": {},
   "source": [
    "Each model type may perform best with different text preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfad6d-0977-4aec-91c5-44085245a27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
